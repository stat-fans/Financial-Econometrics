\documentclass[cn,11pt,chinese]{elegantbook}

\title{金融计量学}
%\subtitle{Elegant\LaTeX{} 经典之作}

\author{徐秋华}
\institute{西南财经大学金融学院}
\date{}
\version{0.1}
\bioinfo{联系方式}{xuqh@swufe.edu.cn}

%\extrainfo{温柔正确的人总是难以生存，因为这世界既不温柔，也不正确。—— 比企谷八幡}

\logo{financialmodeling}
\cover{cover2.jpg}

% 本文档命令
\usepackage{array}
\usepackage{natbib}
\usepackage{mathrsfs}
\newcommand{\ccr}[1]{\makecell{{\color{#1}\rule{1cm}{1cm}}}}

\begin{document}

\maketitle
\frontmatter
\newcommand\specialsectioning{\setcounter{secnumdepth}{-2}}

\chapter*{前言}
\markboth{Introduction}{前言}

金融计量学是金融学科各专业的重要核心课程之一，为学生分析金融数据和解决具体实证金融问题提供了必要的方法论基础。本书是在西南财经大学金融学院本科生课程《金融计量学》和博士生课程《高级计量经济学》的讲义基础上进一步写作完成的。学习本书需要学生具备微积分、线性代数和概率论与数理统计的知识。初级计量经济学的知识对阅读本书来说不是必须的，但是对理解书中的内容是有益的。此外，学生应该具有一定的编程能力，本书根据各章节的特点，选择最适合的编程语言实现相应的计量方法，微观计量部分主要使用Stata，时间序列分析部分主要使用R，机器学习或深度学习等非线性方法部分主要使用Python，本书附录提供了三种编程语言基本知识的介绍。





\vskip 1.5cm

\begin{flushright}
徐秋华\\
于成都$\cdot$西财柳林校区\\
2022年4月
\end{flushright}

\tableofcontents
%\listofchanges

\mainmatter

\specialsectioning
\chapter{第一部分~~金融计量学基础}
\setcounter{secnumdepth}{1}
\chapter{条件期望、条件方差和线性投影}
\begin{introduction}
  \item 条件期望~\ref{def:condexp}
  \item 简单迭代期望律~\ref{thm:slie}
  \item 迭代期望律~\ref{thm:lie}
  \item 条件方差~\ref{def:condvar}
  \item 方差分解~\ref{thm:dv}
  \item 线性投影~\ref{def:linearproj}
\end{introduction}
\bigskip

条件期望在金融计量学中扮演着重要的角色，经典线性回归模型的核心估计方法\textcolor{magenta}{最小二乘（Least squares）}本质上是在估计以解释变量$\bold{x}$为条件被解释变量$\rm{y}$的条件期望。而且，根据$\bold{x}$提供的信息，关于$\rm{y}$的最好（最小化均方误差）的预测也正是条件期望$\mathbb{E}(\rm{y}|\bold{x})$。因此，本章将对条件期望的定义及其性质做一个较为完整的梳理。同时，本章还将讨论条件方差以及条件期望的最优线性近似（也就是：\textcolor{magenta}{线性投影}）的定义和性质。想深入了解本章内容可以进一步参考\cite{angrist2008mostly,bruce2022,wooldridge2010econometric}。

\section{条件期望}
\begin{definition}{条件期望}{condexp}
令$\bold{x}$为$K$维随机向量，$\rm{y}$为一元随机变量。以$\bold{x}$为条件，$\rm{y}$的条件期望函数(Conditional Expectation Function, CEF)定义如下：
	\begin{eqnarray*}
		\mu(\bold{x})=\mathbb{E}(\rm{y}|\bold{x})=\begin{cases}
			\int_{\rm{y}}y\cdot f(y|\bold{x})dy, & \mbox{$\rm{y}$连续,}\\
			\sum_{\rm{y}}y\cdot \mathbb{P}_{\rm{y}|\bold{x}}(y|\bold{x}), & \mbox{$\rm{y}$离散,}
		\end{cases}
	\end{eqnarray*}
	其中，$f(y|\bold{x})$是以$\bold{x}$为条件${\rm{y}}=y$的条件概率密度函数，$\mathbb{P}_{\rm{y}|\bold{x}}(y|\bold{x})$是以$\bold{x}$为条件${\rm{y}}=y$的条件概率。
\end{definition}	
在定义\ref{def:condexp}中，条件期望函数被记为$\mu(\bold{x})$，这表明CEF是随机向量$\bold{x}$的函数，与随机变量$\rm{y}$的具体实现值无关。关于条件期望，我们有如下一些重要的结论：
\begin{theorem}{简单迭代期望律(Simple Law of Iterated Expectations, SLIE)}{slie}
	若$\mathbb{E}|\rm{y}|<\infty$，则对任意的随机向量$\bold{x}$，我们有：
	\begin{eqnarray*}
		\mathbb{E}_{\mathbf{x}}[\mathbb{E}[\mathrm{y}|\mathbf{x}]]=\mathbb{E}[\mathrm{y}],
	\end{eqnarray*}
	其中，符号$\mathbb{E}_{\mathbf{x}}[\cdot]$表示相对于随机向量$\bold{x}$求期望。
\end{theorem}
\begin{proof}
	假设$\rm{y}$和$\bold{x}$的联合概率密度函数为$f(y,\boldsymbol{x})$，因为$\mathbb{E}[\mathrm{y}|\mathbf{x}]$是$\mathbf{x}$的函数，我们有：
	\begin{eqnarray*}
		\mathbb{E}[\mathbb{E}[\mathrm{y} | \mathbf{x}]]=\int_{\mathbb{R}^{k}} \mathbb{E}[\mathrm{y} | \boldsymbol{x}] f_{\mathbf{x}}(\boldsymbol{x}) d \boldsymbol{x}.
	\end{eqnarray*}
	将$\mathrm{y}$为连续随机变量情况下条件期望的定义\ref{def:condexp}代入上式得到：
	\begin{eqnarray*}
		\int_{\mathbb{R}^{k}}\left(\int_{\mathbb{R}} y f(y|\boldsymbol{x}) d y\right) f_{\mathbf{x}}(\boldsymbol{x})  d \boldsymbol{x}=\int_{\mathbb{R}^{k}} \int_{\mathbb{R}} y f(y, \boldsymbol{x}) d y d \boldsymbol{x}=\int_{\mathbb{R}}yf(y)dy=\mathbb{E}[\mathrm{y}],
	\end{eqnarray*}
	其中，第一个等号成立是因为$f(y|\boldsymbol{x}) f_{\mathbf{x}}(\boldsymbol{x})=f(y, \boldsymbol{x})$；第二个等号成立是因为联合分布和边缘分布之间存在如下关系：$\int_{\mathbb{R}^{k}}f(y,\boldsymbol{x})d\boldsymbol{x}=f(y)$。\hfill$\square$
\end{proof}
为了更好地理解定理\ref{thm:slie}，我们假设$\bold{x}$是离散随机变量，则有：
\begin{eqnarray*}
	\mathbb{E}[\mathbb{E}[\mathrm{y} | \mathbf{x}]]=\sum_{j=1}^{\infty} \mathbb{E}\left[\mathrm{y}| \mathbf{x}=\boldsymbol{x}_{j}\right] \mathbb{P}\left[\mathbf{x}=\boldsymbol{x}_{j}\right].
\end{eqnarray*}
如果我们将求期望理解为求平均，那么条件均值的加权（以$\bold{x}$取不同值的概率作为权重）平均是无条件均值。例如，令$\bold{x}$代表性别，$\mathrm{y}$代表身高，
\begin{eqnarray*}
	\mathbb{E}[\mbox{身高}\mid \mbox{性别}=\mbox{女}]\mathbb{P}\left[\mbox{性别}=\mbox{女}\right]+\mathbb{E}[\mbox{身高}\mid \mbox{性别}=\mbox{男}]\mathbb{P}\left[\mbox{性别}=\mbox{男}\right]=\mathbb{E}[\mbox{身高}].
\end{eqnarray*}
所以，如果我们想得到一个人口样本的平均身高，可以直接求平均身高，也可以分别先求得女性和男性的平均身高再以各个性别所占的比例作为权重取加权平均。
\begin{theorem}{迭代期望律(Law of Iterated Expectations, LIE)}{lie}
	若$\mathbb{E}|\rm{y}|<\infty$，则对任意的随机向量$\bold{x}_{1}$和$\bold{x}_{2}$，我们有：
	\begin{eqnarray*}
		\mathbb{E}_{\mathbf{x}_{1}}[\mathbb{E}[\mathrm{y}|\mathbf{x}_{1},\mathbf{x}_{2}]|\mathbf{x}_{1}]=\mathbb{E}[\mathrm{y}|\mathbf{x}_{1}].
	\end{eqnarray*}
\end{theorem}
\begin{proof}
	为了简化证明，这里假设定理中所涉及的随机变量具有联合密度函数。注意到
	\begin{eqnarray}\label{eq1.1}
		f\left(y \mid \boldsymbol{x}_{1}, \boldsymbol{x}_{2}\right) f\left(\boldsymbol{x}_{2} \mid \boldsymbol{x}_{1}\right)=\frac{f\left(y, \boldsymbol{x}_{1}, \boldsymbol{x}_{2}\right)}{f\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}\right)} \frac{f\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}\right)}{f\left(\boldsymbol{x}_{1}\right)}=f\left(y, \boldsymbol{x}_{2} \mid \boldsymbol{x}_{1}\right),
	\end{eqnarray}
	以及，
	\begin{eqnarray}\label{eq1.2}
		\mathbb{E}[\mathrm{y}|\mathbf{x}_{1}=\boldsymbol{x}_{1},\mathbf{x}_{2}=\boldsymbol{x}_{2}]=\int_{\mathbb{R}}yf(y\mid \boldsymbol{x}_{1}, \boldsymbol{x}_{2})dy.
	\end{eqnarray}
	令随机向量$\bold{x}_{2}$的维度为$k_2$，于是，我们有：
	\begin{eqnarray*}
		\mathbb{E}[\mathbb{E}[\mathrm{y}|\bold{x}_{1},\bold{x}_{2}]|\bold{x}_{1}=\boldsymbol{x}_{1}]&=&\int_{\mathbb{R}^{k_{2}}}\mathbb{E}[\mathrm{y}|\mathbf{x}_{1}=\boldsymbol{x}_{1},\mathbf{x}_{2}=\boldsymbol{x}_{2}]f(\boldsymbol{x}_{2}\mid \boldsymbol{x}_{1})d\boldsymbol{x}_{2}\\
		&=&\int_{\mathbb{R}^{k_{2}}}\left(\int_{\mathbb{R}}yf(y\mid \boldsymbol{x}_{1}, \boldsymbol{x}_{2})dy\right)f(\boldsymbol{x}_{2}\mid \boldsymbol{x}_{1})d\boldsymbol{x}_{2}\\
		&=&\int_{\mathbb{R}^{k_{2}}}\int_{\mathbb{R}}yf(y\mid \boldsymbol{x}_{1}, \boldsymbol{x}_{2})f(\boldsymbol{x}_{2}\mid \boldsymbol{x}_{1})dyd\boldsymbol{x}_{2}\\
		&=&\int_{\mathbb{R}^{k_{2}}}\int_{\mathbb{R}}yf\left(y, \boldsymbol{x}_{2} \mid \boldsymbol{x}_{1}\right)dyd\boldsymbol{x}_{2}\\
		&=&\mathbb{E}[\mathrm{y}|\bold{x}_{1}=\boldsymbol{x}_{1}],
	\end{eqnarray*}
	其中，第二个等式用到了式(\ref{eq1.2})；第四个等式用到了式(\ref{eq1.1})。
	
	由上式易知，$\mathbb{E}[\mathbb{E}[\mathrm{y}|\mathbf{x}_{1},\mathbf{x}_{2}]|\mathbf{x}_{1}]=\mathbb{E}[\mathrm{y}|\mathbf{x}_{1}]$。\hfill$\square$
\end{proof}
为了便于记住定理\ref{thm:lie}，我们可以把此性质总结为“小信息集占优”。此定理在时间序列分析中有广泛的应用，例如，令$\mathscr{F}_{t}$表示$t$时刻所能获得的信息，我们有：若$s<t$，$\mathbb{E}\{\mathbb{E}[\mathrm{y}_{t+h}|\mathscr{F}_{t}]|\mathscr{F}_{s}\}=\mathbb{E}[\mathrm{y}_{t+h}|\mathscr{F}_{s}]$。实际上，可以证明如下更一般形式的迭代期望律：令$\bold{w}$为随机向量，$\bold{x}$是随机向量且满足$\bold{x}=\bold{g}(\bold{w})$\footnote{如果已知$\bold{w}$的实现值，$\bold{x}$的实现值便是已知的。}。一般形式的LIE可表示为：
\begin{eqnarray*}
	\mathbb{E}[\rm{y}|\bold{x}]=\mathbb{E}\left[\mathbb{E}(\rm{y}|\bold{w})|\bold{x}\right].
\end{eqnarray*}
\begin{theorem}{条件化定理(Conditioning Theorem, CT)}{ct}
	若$\mathbb{E}|\rm{y}|<\infty$，则，
	\begin{eqnarray*}
	    \mathbb{E}[\textsl{g}(\bold{x})\cdot \rm{y}|\bold{x}]=\textsl{g}(\bold{x})\cdot \mathbb{E}[\rm{y}|\bold{x}].
	\end{eqnarray*}
	若再假设$\mathbb{E}|\textsl{g}(\bold{x})|<\infty$，则，
	\begin{eqnarray*}
		\mathbb{E}[\textsl{g}(\bold{x})\cdot \rm{y}]=\mathbb{E}[\textsl{g}(\bold{x})\cdot \mathbb{E}[\rm{y}|\bold{x}]].
	\end{eqnarray*}
\end{theorem}
\begin{proof}
给定$\bold{x}=\boldsymbol{x}$，我们有：
\begin{eqnarray*}
	&&\mathbb{E}[\textsl{g}({\bold{x}})\cdot {\rm{y}}|{\bold{x}}={\boldsymbol{x}}] = \int_{\mathbb{R}}{\textsl{g}}({\boldsymbol{x}})yf(y|{\boldsymbol{x}})dy={\textsl{g}}({\boldsymbol{x}})\int_{\mathbb{R}}yf(y|{\boldsymbol{x}})dy={\textsl{g}}({\boldsymbol{x}})\mathbb{E}({\rm{y}}|{\bold{x}}={\boldsymbol{x}}).
\end{eqnarray*}
因此，$\mathbb{E}[\textsl{g}(\bold{x})\cdot \rm{y}|\bold{x}]=\textsl{g}(\bold{x})\cdot \mathbb{E}[\rm{y}|\bold{x}]$。由定理\ref{thm:slie}，本定理中的第二个式子容易验证。\hfill$\square$
\end{proof}
下面我们给出关于条件期望函数的两个重要性质：
\begin{property}{（CEF分解性质）}
	对任意的随机变量$\rm{y}$，有如下性质成立：
	\begin{eqnarray*}
	{\rm{y}} = \mathbb{E}[{\rm{y}}|\bold{x}]+\varepsilon,
    \end{eqnarray*}
    其中，
    \begin{itemize}
    	\item[(a)] $\varepsilon$均值独立于$\bold{x}$，即，$\mathbb{E}[\varepsilon|\bold{x}]=0$；
    	\item[(b)] $\varepsilon$与$\bold{x}$的任意函数都不相关。
    \end{itemize}
\end{property}
\begin{proof}
首先证明(a)，
\begin{eqnarray*}
	\mathbb{E}[\varepsilon|\bold{x}]=\mathbb{E}\{\rm{y}- \mathbb{E}[{\rm{y}}|\bold{x}]|\bold{x}\}= \mathbb{E}[{\rm{y}}|\bold{x}]- \mathbb{E}[{\rm{y}}|\bold{x}]=0.
\end{eqnarray*}
其次证明(b)，假设$h(\bold{x})$是$\bold{x}$的任意函数，我们有：
\begin{flalign*}
	\mathrm{Cov}[\varepsilon,h(\bold{x})]&=\mathbb{E}[\varepsilon\cdot h(\bold{x})]-\mathbb{E}(\varepsilon)\mathbb{E}[h(\bold{x})]\\
	&=\mathbb{E}\{\mathbb{E}[\varepsilon\cdot h(\bold{x})|\bold{x}]\}-0\cdot\mathbb{E}[h(\bold{x})] &(\text{这里使用了两次定理\ref{thm:slie}})\\
	&=\mathbb{E}\{h(\bold{x})\mathbb{E}[\varepsilon|\bold{x}]\} &(\text{这里使用了定理\ref{thm:ct}})\\
	&=0. &(\text{这里使用了(a)的结论})
\end{flalign*}\hfill$\square$
\end{proof}
\begin{property}{（CEF预测性质）}
	假设$\mathbb{E}({\rm{y}}^{2})<\infty$，记$\mu(\bold{x})\equiv \mathbb{E}({\rm{y}}|\bold{x})$，则$\mu$是如下优化问题的解：
	\begin{eqnarray*}
	\underset{m\in \mathscr{M}}{\min} \mathbb{E}[({\rm{y}}-m(\bold{x}))^{2}],
	\end{eqnarray*}
	其中，$\mathscr{M}$是函数类，$m:\mathbb{R}^{K}\to \mathbb{R}$满足$\mathbb{E}[m(\bold{x})^{2}]<\infty$。
\end{property}
\begin{proof}
首先，我们对优化的目标函数进行分解：
\begin{eqnarray*}
	&&\mathbb{E}[({\rm{y}}-m(\bold{x}))^{2}]\\
	&=&\mathbb{E}[({\rm{y}}-\mu(\bold{x})+\mu(\bold{x})-m(\bold{x}))^{2}]\\
	&=&\mathbb{E}[({\rm{y}}-\mu(\bold{x}))^{2}]+\mathbb{E}[(\mu(\bold{x})-m(\bold{x}))^{2}]+2\cdot \mathbb{E}\{[{\rm{y}}-\mu(\bold{x})]\cdot[\mu(\bold{x})-m(\bold{x})]\}.
\end{eqnarray*}
注意到，我们需要从函数类$\mathscr{M}$中选择函数$m$使目标函数最小化，上述分解中的第一项与$m$的选择无关，第二项在$m(\bold{x})=\mu(\bold{x})$时取到最小值0，我们只需证明第三项的取值与$m$的选择无关，实际上，
\begin{eqnarray*}
	\mathbb{E}\{[{\rm{y}}-\mu(\bold{x})]\cdot[\mu(\bold{x})-m(\bold{x})]\}
	=\mathbb{E}\{\varepsilon\cdot[\mu(\bold{x})-m(\bold{x})]\}=0,
\end{eqnarray*}
其中，第二个等号成立是因为$\mu(\bold{x})-m(\bold{x})$是$\bold{x}$的函数，利用CEF分解性质(b)即可。

于是，目标函数的最小值在$m(\bold{x})=\mu(\bold{x})$时取到，此性质得证。\hfill$\square$
\end{proof}
对$\bold{x}$任意的函数$m(\bold{x})$，定义其预测$\rm{y}$得到的\textcolor{magenta}{均方误差(mean squared error, MSE)}为
\begin{eqnarray*}
	{\rm{MSE}}({\rm{y}};m)\equiv\mathbb{E}[({\rm{y}}-m(\bold{x}))^{2}],
\end{eqnarray*}
CEF的分解性质表明：对任意的随机变量$\rm{y}$和随机向量$\bold{x}$，$\rm{y}$总是可以分解为$\bold{x}$可以解释的部分$\mathbb{E}[{\rm{y}}|\bold{x}]$和与$\bold{x}$不相关的部分$\varepsilon$。对于$\bold{x}$可以解释的部分$\mathbb{E}[{\rm{y}}|\bold{x}]$，CEF的预测性质指出它是关于$\rm{y}$的最小化均方误差的预测。金融计量学的重要任务之一就是研究如何基于已有的信息对金融时间序列进行预测，因此，条件期望是金融计量学重要的研究对象。值得一提的是，传统的金融计量方法通常假设条件期望函数具有线性函数形式，但实际上$\mathbb{E}[{\rm{y}}|\bold{x}]$可以是$\bold{x}$的非线性函数。为此，金融计量学也发展出诸如参数非线性模型、非参数或半参数模型、神经网络模型等一系列非线性模型。另外，金融计量学还包括公司金融实证研究使用的计量方法，特别是关于因果推断或政策评估的计量方法，这些方法关注的重点不是条件期望，而是一些因果效应参数的估计。
\section{条件方差}
\begin{definition}{条件方差}{condvar}
令$\bold{x}$为随机向量，$\rm{y}$为一元随机变量。如果$\mathbb{E}[\rm{y}^2]<\infty$，以$\bold{x}=\boldsymbol{x}$为条件，$\rm{y}$的条件方差(Conditional Variance)定义如下：
	\begin{eqnarray*}
		\sigma^{2}(\boldsymbol{x})={\mathrm{Var}}[{\rm{y}}|\bold{x}=\boldsymbol{x}]=\mathbb{E}\left[\left({\rm{y}}-\mathbb{E}[\rm{y}|\bold{x}=\boldsymbol{x}]\right)^{2}|\bold{x}=\boldsymbol{x}\right].
	\end{eqnarray*}
	条件方差作为随机变量$\bold{x}$的函数可记为$\sigma^{2}(\bold{x})$。
\end{definition}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{hete.png}
  \caption{条件期望与条件方差}\label{fig1}
\end{figure}
由条件方差的定义可知条件方差也是随机变量$\bold{x}$的函数。给定$\bold{x}$的具体实现值$\boldsymbol{x}$，条件期望$\mathbb{E}[\rm{y}|\bold{x}=\boldsymbol{x}]$计算的是$\rm{y}$的条件均值，而条件方差$\mathrm{Var}[\rm{y}|\bold{x}=\boldsymbol{x}]$度量的是$\rm{y}$的条件分布与其条件均值之间的偏离程度。对于对称的条件分布，条件期望给出了条件分布的中心位置，条件方差刻画的是条件分布偏离中心位置的程度。在图\ref{fig1}中，$\bold{x}$表示受教育年限$educ$，$\rm{y}$表示工资收入$wage$，图中分别给出了受教育年限等于8、12和16年的条件下，工资收入条件分布的均值位置（三条虚线和条件均值曲线$\mathbb{E}[wage|educ]$的交点）以及条件分布的密度函数。我们发现，受教育年限越高，工资收入偏离平均工资收入的程度越大，这种现象在金融计量学中被称为\textcolor{magenta}{条件异方差(Conditional Heteroskedasticity)}。

下面我们给出一些关于条件方差的重要结论：
\begin{theorem}{方差分解(Decomposition of Variance, DV)}{dv}
	若$\mathbb{E}[\rm{y}^{2}]<\infty$，则有：
	\begin{eqnarray*}
		{\mathrm{Var}}[{\rm{y}}] = {\mathbb{E}}\left[{\mathrm{Var}}\left[{\rm{y}}|{\bold{x}}\right]\right]+{\mathrm{Var}}\left[{\mathbb{E}}\left[{\rm{y}}|{\bold{x}}\right]\right].
	\end{eqnarray*}
\end{theorem}
\begin{proof}
	根据CEF分解性质，我们有：
	\begin{flalign*}
		{\mathrm{Var}}[{\rm{y}}]&={\mathrm{Var}}\left\{\mathbb{E}\left[{\rm{y}}|\bold{x}\right]+\varepsilon\right\}\\
		&={\mathrm{Var}}[\mathbb{E}\left[{\rm{y}}|\bold{x}\right]] + {\mathrm{Var}}(\varepsilon) &(\text{$\varepsilon$和$\mathbb{E}[\rm{y}|\bold{x}]$不相关})\\
		&={\mathrm{Var}}[\mathbb{E}\left[{\rm{y}}|\bold{x}\right]] + {\mathbb{E}}(\varepsilon^{2}) &(\text{$\mathbb{E}(\varepsilon)=0$})\\
		&={\mathrm{Var}}[\mathbb{E}\left[{\rm{y}}|\bold{x}\right]] + {\mathbb{E}}[{\mathbb{E}}[\varepsilon^{2}|\bold{x}]] &(\text{由SLIE})\\
		&={\mathrm{Var}}[\mathbb{E}\left[{\rm{y}}|\bold{x}\right]] + {\mathbb{E}}\left[\textcolor{magenta}{{\mathbb{E}}\left[\left[{\rm{y}}-{\mathbb{E}}\left[{\rm{y}}|{\bold{x}}\right]\right]^{2}|\bold{x}\right]}\right] &(\text{$\varepsilon=\rm{y}-\mathbb{E}[\rm{y}|\bold{x}]$})\\
		&={\mathrm{Var}}[\mathbb{E}\left[{\rm{y}}|\bold{x}\right]] + {\mathbb{E}}\left[{\mathrm{Var}}\left[{\rm{y}}|{\bold{x}}\right]\right]. &(\text{根据条件方差的定义})
	\end{flalign*}\hfill$\square$
\end{proof}
方差分解定理表明$\rm{y}$的方差可以分解成$\mathbb{E}[\rm{y}|\bold{x}]$的方差与$\varepsilon$的方差之和，前者是可由$\bold{x}$解释的部分，后者是与$\bold{x}$无关的部分。方差分解定理可以推广到$\rm{y}$的条件方差，我们有如下结论：
\begin{theorem}{条件方差分解(Decomposition of Conditional Variance, DCV)}{dcv}
若$\mathbb{E}[\rm{y}^{2}]<\infty$，则有：
	\begin{eqnarray*}
		{\rm{Var}}({\rm{y}}|\bold{x})=\mathbb{E}[{\rm{Var}}({\rm{y}}|\bold{x},\bold{z})|\bold{x}]+{\rm{Var}}[\mathbb{E}({\rm{y}}|\bold{x},\bold{z})|\bold{x}].
	\end{eqnarray*}
\end{theorem}
\begin{proof}
首先，我们对${\rm{Var}}({\rm{y}}|\bold{x})$进行分解：
		\begin{eqnarray*}
			{\rm{Var}}({\rm{y}}|\bold{x})&\equiv &\mathbb{E}\left\{[{\rm{y}}-\mathbb{E}({\rm{y}}|\bold{x})]^{2}|\bold{x}\right\}\\
			&=&\mathbb{E}\left\{[{\rm{y}}-\mathbb{E}({\rm{y}}|\bold{x},\bold{z})+\mathbb{E}({\rm{y}}|\bold{x},\bold{z})-\mathbb{E}({\rm{y}}|\bold{x})]^{2}|\bold{x}\right\}\\
			&=&\mathbb{E}\left\{[{\rm{y}}-\mathbb{E}({\rm{y}}|\bold{x},\bold{z})]^{2}|\bold{x}\right\}+\mathbb{E}\left\{[\mathbb{E}({\rm{y}}|\bold{x},\bold{z})-\mathbb{E}({\rm{y}}|\bold{x})]^{2}|\bold{x}\right\}\\
			&& +~2\underbrace{\mathbb{E}\left\{[{\rm{y}}-\mathbb{E}({\rm{y}}|\bold{x},\bold{z})]\cdot[\mathbb{E}({\rm{y}}|\bold{x},\bold{z})-\mathbb{E}({\rm{y}}|\bold{x})]|\bold{x}\right\}}_{=~0}.
		\end{eqnarray*}
		由LIE和条件方差的定义，该分解中的第一项可表示为：
		\begin{eqnarray*}
			\mathbb{E}\left\{[{\rm{y}}-\mathbb{E}({\rm{y}}|\bold{x},\bold{z})]^{2}|\bold{x}\right\}=\mathbb{E}\left\{\mathbb{E}\left\{[{\rm{y}}-\mathbb{E}({\rm{y}}|\bold{x},\bold{z})]^{2}|\bold{x},\bold{z}\right\}|\bold{x}\right\}=\mathbb{E}[{\rm{Var}}({\rm{y}}|\bold{x},\bold{z})|\bold{x}].
		\end{eqnarray*}
		同理，第二项可表示为：
		\begin{eqnarray*}
			\mathbb{E}\left\{[\mathbb{E}({\rm{y}}|\bold{x},\bold{z})-\mathbb{E}({\rm{y}}|\bold{x})]^{2}|\bold{x}\right\}=\mathbb{E}\left\{[\mathbb{E}({\rm{y}}|\bold{x},\bold{z})-\mathbb{E}(\mathbb{E}({\rm{y}}|\bold{x},\bold{z})|\bold{x})]^{2}|\bold{x}\right\}={\rm{Var}}[\mathbb{E}({\rm{y}}|\bold{x},\bold{z})|\bold{x}].
		\end{eqnarray*}
		
		第三项为0的推导留作习题，见习题\ref{ex1}。\hfill$\square$
\end{proof}
我们已经知道CEF是关于$\rm{y}$的最小化均方误差的预测，若预测时我们使用更多的信息$(\bold{x},\bold{z})$，相比于$\mathbb{E}[\rm{y}|\bold{x}]$，$\mathbb{E}[\rm{y}|\bold{x},\bold{z}]$是否是关于$\rm{y}$的更好的预测呢？由定理\ref{thm:dcv}，我们容易得到：
\begin{eqnarray*}
	\mathbb{E}[{\rm{Var}}({\rm{y}}|\bold{x})]\geq \mathbb{E}[{\rm{Var}}({\rm{y}}|\bold{x},\bold{z})],
\end{eqnarray*}
其中，
\begin{eqnarray*}
	\mathbb{E}[{\rm{Var}}({\rm{y}}|\bold{x})]=\mathbb{E}\left[ \mathbb{E}\left[\left(\rm{y}-\mathbb{E}\left[\rm{y}|\bold{x}\right]\right)^{2} |\bold{x}\right]\right]=\mathbb{E}\left[\left(\rm{y}-\mathbb{E}\left[\rm{y}|\bold{x}\right]\right)^{2}\right]=\mathrm{MSE}(\rm{y};\mathbb{E}\left[\rm{y}|\bold{x}\right]),
\end{eqnarray*}
同理，
\begin{eqnarray*}
	\mathbb{E}[{\rm{Var}}({\rm{y}}|\bold{x},\bold{z})]=\mathrm{MSE}(\rm{y};\mathbb{E}\left[\rm{y}|\bold{x},\bold{z}\right]).
\end{eqnarray*}
于是，我们有：
\begin{eqnarray*}
	\mathrm{MSE}(\rm{y};\mathbb{E}\left[\rm{y}|\bold{x}\right])\geq \mathrm{MSE}(\rm{y};\mathbb{E}\left[\rm{y}|\bold{x},\bold{z}\right]).
\end{eqnarray*}
从上述分析我们得到$\mathbb{E}\left[\rm{y}|\bold{x},\bold{z}\right]$的MSE不大于$\mathbb{E}\left[\rm{y}|\bold{x}\right]$的MSE，也就是说，从总体(population)的角度讲，基于更多的信息对$\rm{y}$进行预测有助于减少预测的均方误差。除此之外，上述分析与如下定理的结论等价：
\begin{theorem}{方差缩减定理(Variance Reduction Theorem, VRT)}{vrt}
	若$\mathbb{E}[\rm{y}^{2}]<\infty$，则有：
	\begin{eqnarray*}
		\mathrm{Var}[\rm{y}]\geq \mathrm{Var}[\rm{y}-\mathbb{E}[\rm{y}|\bold{x}]]\geq \mathrm{Var}[\rm{y}-\mathbb{E}[\rm{y}|\bold{x},\bold{z}]].
	\end{eqnarray*}
\end{theorem}
\begin{proof}
	记$\varepsilon=\rm{y}-\mathbb{E}[\rm{y}|\bold{x}]$，则有：
	\begin{eqnarray*}
		\mathrm{Var}[\rm{y}-\mathbb{E}[\rm{y}|\bold{x}]]=\mathrm{Var}(\varepsilon)=\mathbb{E}(\varepsilon^{2})=\mathrm{MSE}(\rm{y};\mathbb{E}\left[\rm{y}|\bold{x}\right]).
	\end{eqnarray*}
	同理，
	\begin{eqnarray*}
		\mathrm{Var}[\rm{y}-\mathbb{E}[\rm{y}|\bold{x},\bold{z}]]=\mathrm{MSE}(\rm{y};\mathbb{E}\left[\rm{y}|\bold{x},\bold{z}\right]).
	\end{eqnarray*}
	因此，第二个“$\geq$”得证。由定理\ref{thm:dv}的证明易知$\mathrm{Var}(\rm{y})\geq\mathrm{Var}(\varepsilon)$，第一个“$\geq$”得证。\hfill$\square$
\end{proof}
此定理的结论实际上与我们在下一章定义的拟合优度和$R^2$的概念相对应，不同的是我们这里讨论的是总体的性质，下一章讨论的是样本的性质。

\section{线性投影}
虽然条件期望$\mu(\bold{x})$提供了$\rm{y}$的最优预测，但是$\mu(\bold{x})$的函数形式通常是未知的。在实证分析时，一般假设CEF具有线性形式，但是这种假设往往不切实际。更准确的做法是将线性形式的函数设定看作是关于CEF的近似。为此，我们研究CEF的最优线性近似，即，具有最小均方误差的线性近似。
\begin{definition}{线性投影}{linearproj}
	给定$\bold{x}$，$\rm{y}$的最优线性预测，也称为线性投影，被定义为：
	\begin{eqnarray*}
		\mathscr{P}(\rm{y}|\bold{x})=\bold{x}'\bm{\beta},
	\end{eqnarray*}
	其中，$\bm{\beta}$被称为线性投影系数，它最小化如下均方预测误差：
	\begin{eqnarray*}
		S(\bm{b})=\mathbb{E}\left(({\rm{y}}-\bold{x}'\bm{b})^{2}\right).
	\end{eqnarray*}
	即，
	\begin{eqnarray*}
			\bm{\beta} = \underset{{\bm{b}}\in\mathbb{R}^{K}}{\arg\min}~S(\bm{b}).
	\end{eqnarray*}
\end{definition}
为了推导$\bm{\beta}$的解析表达式，需要施加如下假设：
\begin{assumption}{1.1.}
	\begin{itemize}
		\item[(a)] $\mathbb{E}[\rm{y}^{2}]<\infty$；
		\item[(b)] $\mathbb{E}\|\bold{x}\|^{2}<\infty$；
		\item[(c)] $\boldsymbol{Q}_{\bold{x}\bold{x}}=\mathbb{E}[\bold{x}\bold{x}']$是正定矩阵。
	\end{itemize}
\end{assumption}
\begin{theorem}{线性投影的性质}{prolinearproj}
	\begin{itemize}
		\item[(1)] 线性投影系数$\bm{\beta}$存在且唯一，其表达式为：
		\begin{eqnarray*}
			\bm{\beta} = (\mathbb{E}[\bold{x}\bold{x}'])^{-1}\mathbb{E}[\bold{x}\rm{y}].
		\end{eqnarray*}
		\item[(2)] $\mathscr{P}(\rm{y}|\bold{x})=\bold{x}'(\mathbb{E}[\bold{x}\bold{x}'])^{-1}\mathbb{E}[\bold{x}\rm{y}]$.
		\item[(3)] 投影误差$e=\rm{y}-\bold{x}'\bm{\beta}$存在，并且满足$\mathbb{E}[e^{2}]<\infty$和$\mathbb{E}[\bold{x}e]=\boldsymbol{0}$。特别地，若$\bold{x}$中含有常数项，则$\mathbb{E}(e)=0$。
	\end{itemize}
\end{theorem}
\begin{proof}
	首先，将$S(\bm{b})$进行分解：
	\begin{eqnarray*}
		S(\bm{b}) = \mathbb{E}[\rm{y}^{2}] + \bm{b}'\mathbb{E}[\bold{x}\bold{x}']\bm{b}-2\bm{b}'\mathbb{E}[\bold{x}\rm{y}].
	\end{eqnarray*}
	对上式两端取偏导，由最优化的一阶条件，我们有：
	\begin{eqnarray*}
		\boldsymbol{0} = \frac{\partial S(\bm{b})}{\partial \bm{b}}\big|_{\bm{b}=\bm{\beta}} = -2\mathbb{E}[\bold{x}\rm{y}]+2\mathbb{E}[\bold{x}\bold{x}']\bm{\beta}.
	\end{eqnarray*}
	移项整理得：
	\begin{eqnarray}\label{normaleqn}
		\mathbb{E}[\bold{x}\bold{x}']\bm{\beta} = \mathbb{E}[\bold{x}\rm{y}].
	\end{eqnarray}
	由？？和假设1.1 (b)，我们有：
	\begin{eqnarray*}
		\|\mathbb{E}[\bold{x}\bold{x}']\|\leq \mathbb{E}\|\bold{x}\bold{x}'\|=\mathbb{E}\|\bold{x}\|^{2}<\infty.
	\end{eqnarray*}
	同理，由？？、柯西-施瓦茨不等式和假设1.1，我们有：
	\begin{eqnarray*}
		\|\mathbb{E}[\bold{x}\rm{y}]\|\leq \mathbb{E}\|\bold{x}\rm{y}\|\leq \left(\mathbb{E}\|\bold{x}\|^{2}\right)^{\frac{1}{2}}\left(\mathbb{E}[\rm{y}^{2}]\right)^{\frac{1}{2}}<\infty.
	\end{eqnarray*}
	因此，$\mathbb{E}[\bold{x}\bold{x}']$和$\mathbb{E}[\bold{x}\rm{y}]$是良定义的(well defined)。再由假设1.1 (c)知，$\mathbb{E}[\bold{x}\bold{x}']$可逆，则有：
	\begin{eqnarray}\label{beta}
		\bm{\beta} = (\mathbb{E}[\bold{x}\bold{x}'])^{-1}\mathbb{E}[\bold{x}\rm{y}].
	\end{eqnarray}
	由定义\ref{def:linearproj}，(2)显然成立。
	
	最后，我们证明(3)：
	\begin{flalign*}
		\mathbb{E}(e^{2})&=\mathbb{E}\left[\left(\rm{y}-\bold{x}'\bm{\beta}\right)^{2}\right]\\
		&=\mathbb{E}[\rm{y}^{2}]-2\mathbb{E}[\rm{y}\bold{x}']\bm{\beta}+\bm{\beta}'\mathbb{E}[\bold{x}\bold{x}']\bm{\beta}\\
		&=\mathbb{E}[\rm{y}^{2}]-\mathbb{E}[\rm{y}\bold{x}'](\mathbb{E}[\bold{x}\bold{x}'])^{-1}\mathbb{E}[\bold{x}\rm{y}] &(\text{将式(\ref{beta})的结论代入})\\
		&\leq \mathbb{E}[\rm{y}^{2}] <\infty.&(\text{$(\mathbb{E}[\bold{x}\bold{x}'])^{-1}$半正定})
	\end{flalign*}
	由式(\ref{normaleqn})，我们有：
	\begin{eqnarray*}
		\boldsymbol{0}=\mathbb{E}[\bold{x}{\rm{y}}]-\mathbb{E}[\bold{x}\bold{x}']\bm{\beta}=\mathbb{E}\left[\bold{x}\left({\rm{y}}-\bold{x}'\bm{\beta}\right)\right]=\mathbb{E}\left[{\bold{x}}e\right].
	\end{eqnarray*}
	\hfill$\square$
\end{proof}
定理\ref{thm:prolinearproj}给出了线性投影系数$\bm{\beta}$的一般表达式，接下来我们考虑两种特殊情况：
\begin{itemize}
	\item $\bold{x}=(1,\rm{x}_{1})'$，在此种情况下，我们有：
	\begin{eqnarray*}
		{\rm{y}} = \alpha + \beta_{1}{\rm{x}}_{1} + e,
	\end{eqnarray*}
	其中，$\alpha$和$\beta_{1}$分别是常数项$1$和$\rm{x}_{1}$对应的线性投影系数。容易证明：
	\begin{eqnarray}\label{binary}
		\beta_{1}=\dfrac{{\rm{Cov}}({\rm{y}},\rm{x}_{1})}{{\rm{Var}}(\rm{x}_{1})},\quad \alpha=\mathbb{E}({\rm{y}})-\beta_{1}\mathbb{E}(\rm{x}_{1}).
	\end{eqnarray}
	\item ${\bold{x}}=(1,{\rm{x}}_{1},{\rm{x}}_{2},\cdots,{\rm{x}}_{K})'$，$K>1$，在此种情况下，我们有：
	\begin{eqnarray}\label{anatomy}
			\beta_{k} = \frac{{\rm{Cov}}({\rm{y}},\tilde{\rm{x}}_{k})}{{\rm{Var}}(\tilde{\rm{x}}_{k})},
	\end{eqnarray}
	其中，$\beta_{k}$是${\rm{x}}_{k}$对应的线性投影系数，$\tilde{\rm{x}}_{k}$是${\rm{x}}_{k}$对所有其他协变量投影得到的投影误差。下面我们对式(\ref{anatomy})进行证明。
	\begin{proof}
	首先，将$\rm{y}$写成线性投影加投影误差的形式：
		\begin{eqnarray*}
		{\rm{y}} = \alpha+\beta_{1}{\rm{x}}_{1}+\cdots+\beta_{k}{\rm{x}}_{k}+\cdots+\beta_{K}{\rm{x}}_{K}+e.
	    \end{eqnarray*}
	    将上式代入${\rm{Cov}}({\rm{y}},\tilde{{\rm{x}}}_{k})$，注意到，由习题\ref{ex4}，我们有：
	    \begin{itemize}
		\item ${\rm{Cov}}(\tilde{{\rm{x}}}_{k},e)=0$，因为$\tilde{{\rm{x}}}_{k}$是$\bold{x}$各个分量的线性组合，$e$与$\bold{x}$不相关；
		\item ${\rm{Cov}}(\tilde{{\rm{x}}}_{k},{\rm{x}}_{j})=0,~j\neq k$，因为$\tilde{{\rm{x}}}_{k}$是线性投影误差，与${\rm{x}}_{j}$不相关；
		\item ${\rm{Cov}}(\tilde{{\rm{x}}}_{k},{\rm{x}}_{k})={\rm{Cov}}(\tilde{{\rm{x}}}_{k},\tilde{{\rm{x}}}_{k}+\bold{x}_{-k}'\bm{\beta}_{-k})={\rm{Var}}(\tilde{{\rm{x}}}_{k})$，其中，$\bold{x}_{-k}$表示除${\rm{x}}_{k}$之外的所有协变量构成的向量，即，$\bold{x}_{-k}=(1,{\rm{x}}_{1},\cdots,{\rm{x}}_{k-1},{\rm{x}}_{k+1},\cdots,{\rm{x}}_{K})'$，$\bm{\beta}_{-k}$是${\rm{x}}_{k}$对$\bold{x}_{-k}$进行投影得到的线性投影系数。
	    \end{itemize}
	    于是，
	\begin{eqnarray*}
		\frac{{\rm{Cov}}({\rm{y}},\tilde{{\rm{x}}}_{k})}{{\rm{Var}}(\tilde{{\rm{x}}}_{k})}=\frac{\beta_{k}{\rm{Var}}(\tilde{{\rm{x}}}_{k})}{{\rm{Var}}(\tilde{{\rm{x}}}_{k})}=\beta_{k}.
	\end{eqnarray*}
	\end{proof}
\end{itemize}
我们把第一种情况称为一元线性投影或简单线性投影，把第二种情况称为多元线性投影。
\begin{problemset}
  \item 证明：定理\ref{thm:dcv}的证明过程中关于${\rm{Var}}({\rm{y}}|\bold{x})$的分解表达式的第三项为0。\label{ex1}
  \item 试用CEF的分解性质证明定理\ref{thm:vrt}。\label{ex2}
  \item 证明：在假设1.1成立的条件下，定理\ref{thm:prolinearproj}中定义的投影误差$e$满足$\mathbb{E}[\bold{x}e]<\infty$。\footnote{不能利用$\mathbb{E}[\bold{x}e]=\boldsymbol{0}$的结论。}
  \item 证明：在定理\ref{thm:prolinearproj}中，若$\bold{x}$中含有常数项，则$\bold{x}$与$e$不相关。\label{ex4}
  \item 证明：利用定理\ref{thm:prolinearproj} (1)的结论证明式(\ref{binary})成立。
\end{problemset}

\chapter{多元线性回归}
\chapter{内生性和工具变量}
\chapter{分位数回归}


\specialsectioning
\chapter{第二部分~~常用估计方法}
\setcounter{secnumdepth}{1}

\chapter{广义矩估计}

\chapter{极大似然估计}

\chapter{非参数估计}

\chapter{贝叶斯估计}

\specialsectioning
\chapter{第三部分~~微观计量方法}
\setcounter{secnumdepth}{1}


\chapter{面板数据模型}

\chapter{离散因变量模型}

\chapter{删失和截尾}

\chapter{事件研究}

\chapter{因果推断的计量方法}

\specialsectioning
\chapter{第四部分~~时间序列分析}
\setcounter{secnumdepth}{1}

\chapter{线性时间序列模型}
\chapter{波动率建模}
\chapter{非线性时间序列模型}

\specialsectioning
\chapter{第五部分~~实证资产定价}
\setcounter{secnumdepth}{1}

\chapter{ }

\specialsectioning
\chapter{第六部分~~量化风险管理}
\setcounter{secnumdepth}{1}

\chapter{相关性建模}
\chapter{单资产风险测度}
\chapter{系统风险测度}


\nocite{*} 
\bibliography{reference}
\appendix

\chapter{矩阵代数}


本附录

\section{向量范数}

\section{矩阵范数}

\textbf{内积}
 
\chapter{最优化基础}

\chapter{常用不等式}

\chapter{Stata入门}
\chapter{R入门}
\chapter{Python入门}

\end{document}
